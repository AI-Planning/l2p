# Config file for llm: https://github.com/simonw/llm 
# Prices vary for respective API providers. Default prices may not reflect current prices.

# [Structure template]
# {PROVIDER}:
#   {MODEL_NAME}:
#     model_family: {MODEL_FAMILY_NAME}
#     model_alias: {MODEL_API_NAME}
#     model_context_length: {MODEL_WINDOW}
#     model_params:
#       {custom parameter arguments}. For example:
#       max_completion_tokens: 
#       temperature:
#       top_p:
#       stop:

# GEMINI
google:
  gemini-3-pro-preview:
    model_family: gemini-3
    model_alias: gemini-3-pro-preview
    model_context_length: 1000000 # max ~1,000,000
    model_params:
      max_output_tokens: 16384 # max: 65536
      temperature: 1.0
      thinking_level: low # low/high
      top_p: 1.0
    cost_usd_mtok:
      input: 2.00
      output: 12.00
  gemini-3-flash-preview:
    model_family: gemini-3
    model_alias: gemini-3-flash-preview
    model_context_length: 1000000 # max ~1,000,000
    model_params:
      max_output_tokens: 16384 # max: 65536
      temperature: 1.0
      thinking_level: low # low/high
      top_p: 1.0
    cost_usd_mtok:
      input: 0.50
      output: 3.00
  gemini-2.5-pro:
    model_family: gemini-2.5
    model_alias: gemini-2.5-pro
    model_context_length: 1000000 # max ~1,000,000
    model_params:
      max_output_tokens: 16384 # max: 65,536
      temperature: 1.0
      thinking_budget: 8192
      top_p: 1.0
    cost_usd_mtok:
      input: 1.25
      output: 10.00
  gemini-2.5-flash:
    model_family: gemini-2.5
    model_alias: gemini-2.5-flash
    model_context_length: 1000000 # max ~1,000,000
    model_params:
      max_output_tokens: 16384 # max: 65,536
      temperature: 1.0
      thinking_budget: 8192
      top_p: 1.0
    cost_usd_mtok:
      input: 0.30
      output: 2.50
  gemini-2.5-flash-lite:
    model_family: gemini-2.5
    model_alias: gemini-2.5-flash-lite
    model_context_length: 1000000 # max ~1,000,000
    model_params:
      max_output_tokens: 16384 # max: 65,536
      temperature: 1.0
      thinking_budget: 8192
      top_p: 1.0
    cost_usd_mtok:
      input: 0.10
      output: 0.40
  gemini-2.0-flash:
    model_family: gemini-2.0
    model_alias: gemini-2.0-flash
    model_context_length: 1000000 # max ~1,000,000
    model_params:
      max_output_tokens: 8192
      temperature: 0.0
      top_p: 1.0
    cost_usd_mtok:
      input: 0.10
      output: 0.40
  gemini-2.0-flash-lite:
    model_family: gemini-2.0
    model_alias: gemini-2.0-flash-lite
    model_context_length: 1000000 # max ~1,000,000
    model_params:
      max_output_tokens: 8192
      temperature: 0.0
      top_p: 1.0
    cost_usd_mtok:
      input: 0.075
      output: 0.30

# OPENAI
openai:
  gpt-5.2:
    model_family: gpt-5
    model_alias: gpt-5.2
    model_context_length: 400000
    model_params:
      # max_tokens: 8192 # max: 128k
      temperature: 1.0
      top_p: 1.0
      frequency_penalty: 0.0
      presence_penalty: 0.0
      reasoning_effort: medium
    cost_usd_mtok:
      input: 1.75
      output: 14.00
  gpt-5.1:
    model_family: gpt-5
    model_alias: gpt-5.1
    model_context_length: 400000
    model_params:
      # max_tokens: 8192 # max: 128k
      temperature: 1.0
      top_p: 1.0
      frequency_penalty: 0.0
      presence_penalty: 0.0
      reasoning_effort: medium
    cost_usd_mtok:
      input: 1.25
      output: 10.00
  gpt-5:
    model_family: gpt-5
    model_alias: gpt-5
    model_context_length: 400000
    model_params:
      # max_tokens: 8192 # max: 128k
      temperature: 1.0
      top_p: 1.0
      frequency_penalty: 0.0
      presence_penalty: 0.0
      reasoning_effort: medium
    cost_usd_mtok:
      input: 1.25
      output: 10.00
  gpt-5-mini:
    model_family: gpt-5
    model_alias: gpt-5-mini
    model_context_length: 400000
    model_params: 
      # max_tokens: 8192 # max: 128k
      temperature: 1.0
      top_p: 1.0
      frequency_penalty: 0.0
      presence_penalty: 0.0
      reasoning_effort: medium
    cost_usd_mtok:
      input: 0.25
      output: 2.00
  gpt-5-nano:
    model_family: gpt-5
    model_alias: gpt-5-nano
    model_context_length: 400000
    model_params:
      # max_tokens: 8192 # max: 128k
      temperature: 1.0
      top_p: 1.0
      frequency_penalty: 0.0
      presence_penalty: 0.0
      reasoning_effort: medium
    cost_usd_mtok:
      input: 0.05
      output: 0.40
  o1:
    model_family: o1
    model_alias: o1
    model_context_length: 200000
    model_params:
      # max_tokens: 8192 # max: 100k
      temperature: 1.0
      top_p: 1.0
      frequency_penalty: 0.0
      presence_penalty: 0.0
      stop: None,
      reasoning_effort: medium
    cost_usd_mtok:
      input: 15.00
      output: 60.00
  o1-mini:
    model_family: o1
    model_alias: o1-mini
    model_context_length: 128000
    model_params:
      # max_completion_tokens: 8192 # max: 65536
      temperature: 1.0
      top_p: 1.0
      frequency_penalty: 0.0
      presence_penalty: 0.0
      stop:
    cost_usd_mtok:
      input: 1.10
      output: 4.40
  o3:
    model_family: o3
    model_alias: o3
    model_context_length: 200000
    model_params:
      # max_completion_tokens: 8192 # max: 100k
      temperature: 1.0
      top_p: 1.0
      frequency_penalty: 0.0
      presence_penalty: 0.0
      stop:
      reasoning_effort: medium
    cost_usd_mtok:
      input: 2.00
      output: 8.00
  o3-mini:
    model_family: o3
    model_alias: o3-mini
    model_context_length: 200000
    model_params:
      # max_completion_tokens: 8192 # max: 100k
      temperature: 1.0
      top_p: 1.0
      frequency_penalty: 0.0
      presence_penalty: 0.0
      stop:
      reasoning_effort: medium
    cost_usd_mtok:
      input: 1.10
      output: 4.40
  o4-mini:
    model_family: o4
    model_alias: o4-mini
    model_context_length: 200000
    model_params:
      # max_completion_tokens: 8192 # max: 100k
      temperature: 1.0
      top_p: 1
      frequency_penalty: 0.0
      presence_penalty: 0.0
      stop:
      reasoning_effort: medium
    cost_usd_mtok:
      input: 1.10
      output: 4.40
  gpt-4.1:
    model_family: gpt-4.1
    model_alias: gpt-4.1
    model_context_length: 1047575
    model_params:
      max_tokens: 8192 # max: 32768
      temperature: 0.0
      top_p: 1.0
      frequency_penalty: 0.0
      presence_penalty: 0.0
      stop:
    cost_usd_mtok:
      input: 2.00
      output: 8.00
  gpt-4.1-mini:
    model_family: gpt-4.1
    model_alias: gpt-4.1-mini
    model_context_length: 1047575
    model_params:
      max_tokens: 8192 # max: 32768
      temperature: 0.0
      top_p: 1.0
      frequency_penalty: 0.0
      presence_penalty: 0.0
      stop: None
    cost_usd_mtok:
      input: 0.40
      output: 1.60
  gpt-4.1-nano:
    model_family: gpt-4.1
    model_alias: gpt-4.1-nano
    model_context_length: 1047575
    model_params:
      max_tokens: 8192 # max: 32768
      temperature: 0.0
      top_p: 1.0
      frequency_penalty: 0.0
      presence_penalty: 0.0
      stop:
    cost_usd_mtok:
      input: 0.10
      output: 0.40
  gpt-4o:
    model_family: gpt-4o
    model_alias: gpt-4o
    model_context_length: 128000
    model_params:
      max_tokens: 8192 # max: 16384
      temperature: 0.0
      top_p: 1.0
      frequency_penalty: 0.0
      presence_penalty: 0.0
      stop:
    cost_usd_mtok:
      input: 2.50
      output: 10.00
  gpt-4o-mini:
    model_family: gpt-4o
    model_alias: gpt-4o-mini
    model_context_length: 128000
    model_params:
      max_tokens: 8192 # max: 16384
      temperature: 0.0
      top_p: 1.0
      frequency_penalty: 0.0
      presence_penalty: 0.0
      stop:
    cost_usd_mtok:
      input: 0.15
      output: 0.60
  gpt-4:
    model_family: gpt-4
    model_alias: gpt-4
    model_context_length: 8192
    model_params:
      max_tokens: 4096
      temperature: 0.0
      top_p: 1.0
      frequency_penalty: 0.0
      presence_penalty: 0.0
      stop:
    cost_usd_mtok:
      input: 30.00
      output: 60.00
  gpt-4-turbo:
    model_family: gpt-4
    model_alias: gpt-4-turbo
    model_context_length: 128000
    model_params:
      max_tokens: 4096
      temperature: 0.0
      top_p: 1.0
      frequency_penalty: 0.0
      presence_penalty: 0.0
      stop:
    cost_usd_mtok:
      input: 10.00
      output: 30.00
  gpt-3.5-turbo:
    model_family: gpt-3.5
    model_alias: gpt-3.5-turbo
    model_context_length: 16385
    model_params:
      max_tokens: 4096
      temperature: 0.0
      top_p: 1.0
      frequency_penalty: 0.0
      presence_penalty: 0.0
      stop:
    cost_usd_mtok:
      input: 0.50
      output: 1.50

# DEEPSEEK
deepseek:
  deepseek-coder:
    model_family: deepseek
    model_alias: deepseek-coder
    model_context_length: 128000
    model_params:
      max_tokens: 4096
      temperature: 0.0
      top_p: 1.0
      stop:
    cost_usd_mtok:
      input: 0.028
      output: 0.42
  deepseek-chat:
    model_family: deepseek
    model_alias: deepseek-chat
    model_context_length: 128000
    model_params:
      max_tokens: 8192 # max 8k
      temperature: 0.0
      top_p: 1.0
      stop:
    cost_usd_mtok:
      input: 0.028
      output: 0.42
  deepseek-reasoner:
    model_family: deepseek
    model_alias: deepseek-reasoner
    model_context_length: 128000
    model_params:
      max_tokens: 8192 # max 64k
      temperature: 0.0
      top_p: 1.0
      stop:
    cost_usd_mtok:
      input: 0.028
      output: 0.42

# ANTHROPIC
anthropic:
  claude-opus-4.5:
    model_family: opus
    model_alias: claude-opus-4.5
    model_context_length: 200000
    model_params:
      max_tokens: 16384 # max: 64k
      temperature: 1.0 # temp must equal 1 when thinking enabled
      thinking: True
      thinking_budget: 8192
    cost_usd_mtok:
      input: 5.00
      output: 25.00
  claude-opus-4.1:
    model_family: opus
    model_alias: claude-opus-4.1
    model_context_length: 200000
    model_params:
      max_tokens: 16384 # max: 32k
      temperature: 1.0 # temp must equal 1 when thinking enabled
      thinking: True
      thinking_budget: 8192
    cost_usd_mtok:
      input: 15.00
      output: 75.00
  claude-4-opus:
    model_family: opus
    model_alias: claude-4-opus
    model_context_length: 200000
    model_params:
      max_tokens: 16384 # max: 32k
      temperature: 1.0 # temp must equal 1 when thinking enabled
      thinking: True
      thinking_budget: 8192
    cost_usd_mtok:
      input: 15.00
      output: 75.00
  claude-sonnet-4.5:
    model_family: sonnet
    model_alias: claude-sonnet-4.5
    model_context_length: 200000
    model_params:
      max_tokens: 16384 # max: 160k
      temperature: 1.0 # temp must equal 1 when thinking enabled
      thinking: True
      thinking_budget: 8192
    cost_usd_mtok:
      input: 3.00
      output: 15.00
  claude-4-sonnet:
    model_family: sonnet
    model_alias: claude-4-sonnet
    model_context_length: 200000
    model_params:
      max_tokens: 16384 # max: 64k
      temperature: 1.0 # temp must equal 1 when thinking enabled
      thinking: True
      thinking_budget: 8192
    cost_usd_mtok:
      input: 3.00
      output: 15.00
  claude-3.7-sonnet: # deprecated
    model_family: sonnet
    model_alias: claude-3.7-sonnet
    model_context_length: 200000
    model_params:
      max_tokens: 8192 # max: 128k
      temperature: 0.0 # only one of top_p or temperature can be set
      # top_p: 1.0
    cost_usd_mtok:
      input: 3.00
      output: 15.00
  claude-3.5-sonnet:
    model_family: sonnet
    model_alias: claude-3.5-sonnet
    model_context_length: 200000
    model_params:
      max_tokens: 4096
      temperature: 0.0 # only one of top_p or temperature can be set
      # top_p: 1.0
    cost_usd_mtok:
      input: 3.00
      output: 15.00
  claude-haiku-4.5:
    model_family: haiku
    model_alias: claude-haiku-4.5
    model_context_length: 200000
    model_params:
      max_tokens: 16384 # max: 64k
      temperature: 1.0 # temp must equal 1 when thinking enabled
      thinking: True
      thinking_budget: 8192
    cost_usd_mtok:
      input: 1.00
      output: 5.00
  claude-3.5-haiku:
    model_family: haiku
    model_alias: claude-3.5-haiku
    model_context_length: 200000
    model_params:
      max_tokens: 4096
      temperature: 0.0 # only one of top_p or temperature can be set
      # top_p = 1.0
    cost_usd_mtok:
      input: 0.80
      output: 4.00
  claude-3-haiku:
    model_family: haiku
    model_alias: claude-3-haiku
    model_context_length: 200000
    model_params:
      max_tokens: 4096
      temperature: 0.0 # only one of top_p or temperature can be set
      # top_p: 1.0
    cost_usd_mtok:
      input: 0.25
      output: 1.25

# MISTRAL
mistral:
  mistral-large:
    model_family: mistral
    model_alias: mistral-large
    model_context_length: 256000
    model_params:
      max_tokens: 8192
      temperature: 0.0
      top_p: 1.0
    cost_usd_mtok:
      input: 0.50
      output: 1.50
  mistral-medium:
    model_family: mistral
    model_alias: mistral-medium
    model_context_length: 128000
    model_params:
      max_tokens: 8192
      temperature: 0.0
      top_p: 1.0
    cost_usd_mtok:
      input: 0.40
      output: 2.00
  mistral-small:
    model_family: mistral
    model_alias: mistral-small
    model_context_length: 128000
    model_params:
      max_tokens: 8192
      temperature: 0.0
      top_p: 1.0
    cost_usd_mtok:
      input: 0.10
      output: 0.30
  ministral-8b:
    model_family: ministral
    model_alias: ministral-8b
    model_context_length: 256000
    model_params:
      max_tokens: 8192
      temperature: 0.0
      top_p: 1.0
    cost_usd_mtok:
      input: 0.15
      output: 0.15
  ministral-3b:
    model_family: ministral
    model_alias: ministral-3b
    model_context_length: 256000
    model_params:
      max_tokens: 8192
      temperature: 0.0
      top_p: 1.0
    cost_usd_mtok:
      input: 0.10
      output: 0.10
  codestral:
    model_family: codestral
    model_alias: codestral
    model_context_length: 128000
    model_params:
      max_tokens: 8192
      temperature: 0.0
      top_p: 1.0
    cost_usd_mtok:
      input: 0.30
      output: 0.90

# max tokens == num_predict
# reasoning level = medium (~8192 budget)
ollama:
  gpt-oss:120b:
    model_family: gpt-oss
    model_alias: gpt-oss:120b
    model_context_length: 128000
    model_params:
      num_predict: 8192 # max 128000
      temperature: 0.0
      top_p: 1.0
      top_k: 0.0
      repeat_penalty: 1.0
  gpt-oss:20b:
    model_family: gpt-oss
    model_alias: gpt-oss:20b
    model_context_length: 128000
    model_params:
      num_predict: 8192 # max 128000
      temperature: 0.0
      top_p: 1.0
      top_k: 0.0
      repeat_penalty: 1.0

  deepseek-r1:671b:
    model_family: deepseek-reasoner
    model_alias: deepseek-r1:671b
    model_context_length: 160000
    model_params:
      num_predict: 8192
      temperature: 0.0
      top_p: 1.0
      top_k: 0.0
      repeat_penalty: 1.0
  deepseek-r1:70b:
    model_family: deepseek-reasoner
    model_alias: deepseek-r1:70b
    model_context_length: 128000
    model_params:
      num_predict: 8192
      temperature: 0.0
      top_p: 1.0
      top_k: 0.0
      repeat_penalty: 1.0
  deepseek-r1:32b:
    model_family: deepseek-reasoner
    model_alias: deepseek-r1:32b
    model_context_length: 128000
    model_params:
      num_predict: 8192
      temperature: 0.0
      top_p: 1.0
      top_k: 0.0
      repeat_penalty: 1.0
  deepseek-r1:14b:
    model_family: deepseek-reasoner
    model_alias: deepseek-r1:14b
    model_context_length: 128000
    model_params:
      num_predict: 8192
      temperature: 0.0
      top_p: 1.0
      top_k: 0.0
      repeat_penalty: 1.0
  deepseek-r1:8b:
    model_family: deepseek-reasoner
    model_alias: deepseek-r1:8b
    model_context_length: 128000
    model_params:
      num_predict: 8192
      temperature: 0.0
      top_p: 1.0
      top_k: 0.0
      repeat_penalty: 1.0
  deepseek-v3:671b:
    model_family: deepseek-chat
    model_alias: deepseek-v3:671b
    model_context_length: 160000
    model_params:
      num_predict: 8192
      temperature: 0.0
      top_p: 1.0
      top_k: 0.0
      repeat_penalty: 1.0
  deepseek-coder:33b:
    model_family: deepseek-coder
    model_alias: deepseek-coder:33b
    model_context_length: 16000
    model_params:
      num_predict: 8192
      temperature: 0.0
      top_p: 1.0
      top_k: 0.0
      repeat_penalty: 1.0
  deepseek-coder:6.7b:
    model_family: deepseek-coder
    model_alias: deepseek-coder:6.7b
    model_context_length: 16000
    model_params:
      num_predict: 8192
      temperature: 0.0
      top_p: 1.0
      top_k: 0.0
      repeat_penalty: 1.0
  deepseek-coder:1.3b:
    model_family: deepseek-coder
    model_alias: deepseek-coder:1.3b
    model_context_length: 16000
    model_params:
      num_predict: 8192
      temperature: 0.0
      top_p: 1.0
      top_k: 0.0
      repeat_penalty: 1.0

  mixtral:8x22b:
    model_family: mixtral
    model_alias: mixtral:8x22b
    model_context_length: 64000
    model_params:
      num_predict: 8192
      temperature: 0.0
      top_p: 1.0
      top_k: 0.0
      repeat_penalty: 1.0
  mixtral:8x7b:
    model_family: mixtral
    model_alias: mixtral:8x7b
    model_context_length: 32000
    model_params:
      num_predict: 8192
      temperature: 0.0
      top_p: 1.0
      top_k: 0.0
      repeat_penalty: 1.0

  llama3.2:3b:
    model_family: llama3.2
    model_alias: llama3.2:3b
    model_context_length: 128000
    model_params:
      num_predict: 8192
      temperature: 0.0
      top_p: 1.0
      top_k: 0.0
      repeat_penalty: 1.0
  llama3.2:1b:
    model_family: llama3.2
    model_alias: llama3.2:1b
    model_context_length: 128000
    model_params:
      num_predict: 8192
      temperature: 0.0
      top_p: 1.0
      top_k: 0.0
      repeat_penalty: 1.0
  llama3.1:405b:
    model_family: llama3.1
    model_alias: llama3.1:405b
    model_context_length: 128000
    model_params:
      num_predict: 8192
      temperature: 0.0
      top_p: 1.0
      top_k: 0.0
      repeat_penalty: 1.0
  llama3.1:70b:
    model_family: llama3.1
    model_alias: llama3.1:70b
    model_context_length: 128000
    model_params:
      num_predict: 8192
      temperature: 0.0
      top_p: 1.0
      top_k: 0.0
      repeat_penalty: 1.0
  llama3.1:8b:
    model_family: llama3.1
    model_alias: llama3.1:8b
    model_context_length: 128000
    model_params:
      num_predict: 8192
      temperature: 0.0
      top_p: 1.0
      top_k: 0.0
      repeat_penalty: 1.0
  llama3:70b:
    model_family: llama2
    model_alias: llama3:70b
    model_context_length: 8000
    model_params:
      num_predict: 4096
      temperature: 0.0
      top_p: 1.0
      top_k: 0.0
      repeat_penalty: 1.0
  llama3:8b:
    model_family: llama3
    model_alias: llama3:8b
    model_context_length: 8000
    model_params:
      num_predict: 4096
      temperature: 0.0
      top_p: 1.0
      top_k: 0.0
      repeat_penalty: 1.0
  llama2:70b:
    model_family: llama2
    model_alias: llama2:70b
    model_context_length: 4000
    model_params:
      num_predict: 1024
      temperature: 0.0
      top_p: 1.0
      top_k: 0.0
      repeat_penalty: 1.0
  llama2:13b:
    model_family: llama2
    model_alias: llama2:13b
    model_context_length: 4000
    model_params:
      num_predict: 1024
      temperature: 0.0
      top_p: 1.0
      top_k: 0.0
      repeat_penalty: 1.0
  llama2:7b:
    model_family: llama2
    model_alias: llama2:7b
    model_context_length: 4000
    model_params:
      num_predict: 1024
      temperature: 0.0
      top_p: 1.0
      top_k: 0.0
      repeat_penalty: 1.0

  qwen3:235b:
    model_family: qwen3
    model_alias: qwen3:235b
    model_context_length: 256000
    model_params:
      num_predict: 8192
      temperature: 0.0
      top_p: 1.0
      top_k: 0.0
      repeat_penalty: 1.0
  qwen3:32b:
    model_family: qwen3
    model_alias: qwen3:32b
    model_context_length: 40000
    model_params:
      num_predict: 8192
      temperature: 0.0
      top_p: 1.0
      top_k: 0.0
      repeat_penalty: 1.0
  qwen3:30b:
    model_family: qwen3
    model_alias: qwen3:30b
    model_context_length: 256000
    model_params:
      num_predict: 8192
      temperature: 0.0
      top_p: 1.0
      top_k: 0.0
      repeat_penalty: 1.0
  qwen3:14b:
    model_family: qwen3
    model_alias: qwen3:14b
    model_context_length: 40000
    model_params:
      num_predict: 8192
      temperature: 0.0
      top_p: 1.0
      top_k: 0.0
      repeat_penalty: 1.0
  qwen3:8b:
    model_family: qwen3
    model_alias: qwen3:8b
    model_context_length: 40000
    model_params:
      num_predict: 8192
      temperature: 0.0
      top_p: 1.0
      top_k: 0.0
      repeat_penalty: 1.0
  qwen2.5:72b:
    model_family: qwen2.5
    model_alias: qwen2.5:72b
    model_context_length: 32000
    model_params:
      num_predict: 8192
      temperature: 0.0
      top_p: 1.0
      top_k: 0.0
      repeat_penalty: 1.0
  qwen2.5:32b:
    model_family: qwen2.5
    model_alias: qwen2.5:32b
    model_context_length: 32000
    model_params:
      num_predict: 8192
      temperature: 0.0
      top_p: 1.0
      top_k: 0.0
      repeat_penalty: 1.0
  qwen2.5:14b:
    model_family: qwen2.5
    model_alias: qwen2.5:14b
    model_context_length: 32000
    model_params:
      num_predict: 8192
      temperature: 0.0
      top_p: 1.0
      top_k: 0.0
      repeat_penalty: 1.0
  qwen2.5:7b:
    model_family: qwen2.5
    model_alias: qwen2.5:7b
    model_context_length: 32000
    model_params:
      num_predict: 8192
      temperature: 0.0
      top_p: 1.0
      top_k: 0.0
      repeat_penalty: 1.0

  command-a:111b:
    model_family: command-a
    model_alias: command-a:111b
    model_context_length: 16000
    model_params:
      num_predict: 8192
      temperature: 0.0
      top_p: 1.0
      top_k: 0.0
      repeat_penalty: 1.0
  command-r:35b:
    model_family: command-r
    model_alias: command-r:35b
    model_context_length: 128000
    model_params:
      num_predict: 8192
      temperature: 0.0
      top_p: 1.0
      top_k: 0.0
      repeat_penalty: 1.0
  aya:35b:
    model_family: aya
    model_alias: aya:35b
    model_context_length: 8000
    model_params:
      num_predict: 4096
      temperature: 0.0
      top_p: 1.0
      top_k: 0.0
      repeat_penalty: 1.0
  aya:8b:
    model_family: aya
    model_alias: aya:8b
    model_context_length: 8000
    model_params:
      num_predict: 4096
      temperature: 0.0
      top_p: 1.0
      top_k: 0.0
      repeat_penalty: 1.0